{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Features Engineering and Implication of SAKT+LightGBM** \n1. Features engineering uncovers many hidden facts that are significant to trace any user.\n2. The self attention knowledge tracing model with feed forward neural network tracks user behavior using the context of interactions.\n3. Multiple heads in self attention knowledge tracing model, for tracing user knowledge level, gives advantage of parallel multiple executions to improve prediction efficiently. \n4. Use of LightGBM package is its fast-speed, high-efficiency, and low memory usage.\n","metadata":{}},{"cell_type":"code","source":"!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > /dev/null 2>&1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import packages\nimport numpy as np \nimport pandas as pd \nimport psutil\nfrom collections import defaultdict\nimport datatable as dt\nimport lightgbm as lgb\nfrom matplotlib import pyplot as plt\nimport riiideducation\nimport random\nfrom sklearn.metrics import roc_auc_score\nimport gc\nimport csv\nimport pickle\n\n_ = np.seterr(divide='ignore', invalid='ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Pre-processing**","metadata":{}},{"cell_type":"code","source":"data_types_dict = {\n    'timestamp': 'int64',\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'content_type_id':'int8', \n    'task_container_id': 'int16',\n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\ntarget = 'answered_correctly'\ntrain_df = dt.fread('../input/riiid-test-answer-prediction/train.csv', columns=set(data_types_dict.keys())).to_pandas()\nprint(psutil.virtual_memory().percent)\ngc.collect()\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dataframe of lectures.csv\nlectures_df = pd.read_csv('/kaggle/input/riiid-test-answer-prediction/lectures.csv')\nprint(lectures_df)\n# Replace \"solving question\" value in type_of column with \"solving_question\"\nlectures_df['type_of'] = lectures_df['type_of'].replace('solving question', 'solving_question')\n# Get dummies of two columns (part, type_of) in lectures_df\nlectures_df = pd.get_dummies(lectures_df, columns=['part', 'type_of'])\nlectures_top_categories = [column for column in lectures_df.columns if column.startswith('part')]\nlectures_basic_purposes = [column for column in lectures_df.columns if column.startswith('type_of_')]\nprint(lectures_df)\n# Merge train_df with lectures_df where content_id is lecture_id\ntrain_df_with_lectures = train_df[train_df.content_type_id == True].merge(lectures_df, left_on='content_id', right_on='lecture_id', how='left')\nprint(train_df_with_lectures)\n# For each user, get total lecture interactions for each part and each type\nuser_lecture_interactions = train_df_with_lectures.groupby('user_id',as_index = False)[lectures_top_categories + lectures_basic_purposes].sum()\nprint(user_lecture_interactions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify datatypes of all newly added columns in lectures_df\nlecturedata_types_dict = {   \n    'user_id': 'int32', \n    'part_1': 'int8',\n    'part_2': 'int8',\n    'part_3': 'int8',\n    'part_4': 'int8',\n    'part_5': 'int8',\n    'part_6': 'int8',\n    'part_7': 'int8',\n    'type_of_concept': 'int8',\n    'type_of_intention': 'int8',\n    'type_of_solving_question': 'int8',\n    'type_of_starter': 'int8'\n}\nuser_lecture_interactions = user_lecture_interactions.astype(lecturedata_types_dict)\nfor column in user_lecture_interactions.columns:\n    if(column !='user_id'):\n        user_lecture_interactions[column] = (user_lecture_interactions[column] > 0).astype('int8')\nprint(user_lecture_interactions.dtypes)\ndel(train_df_with_lectures)\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate user interactions count and pattern of lecture interactions along the records of each user\ncum = train_df.groupby('user_id')['content_type_id'].agg(['cumsum', 'cumcount'])\ntrain_df['cumulative_user_lectures'] = cum['cumsum'] \ntrain_df['user_lecture_interaction_pattern'] = cum['cumsum'] / cum['cumcount']\ntrain_df.user_lecture_interaction_pattern=train_df.user_lecture_interaction_pattern.astype('float16')\ntrain_df.cumulative_user_lectures=train_df.cumulative_user_lectures.astype('int8')\nprint(train_df['cumulative_user_lectures'].head(n=50))\nprint(train_df['user_lecture_interaction_pattern'].head(n=50))\n\n# Calculate user interactions count and total lecture interactions for each user\nuser_lecture_aggregate = train_df.groupby('user_id')['content_type_id'].agg(['sum', 'count'])\nprint(user_lecture_aggregate)\n\n# Specify datatypes of columns in train_df\ndata_types_dict = {\n    'timestamp': 'int64',\n    'user_id': 'int32', \n    'content_id': 'int16', \n    'content_type_id':'int8', \n    'task_container_id': 'int16',\n    'answered_correctly': 'int8', \n    'prior_question_elapsed_time': 'float32', \n    'prior_question_had_explanation': 'bool'\n}\ntarget = 'answered_correctly'\n# Fill NAN places in \"prior_question_had_explanation\" column with False\ntrain_df['prior_question_had_explanation'].fillna(False, inplace=True)\ndel cum\ngc.collect()\n# Set specified datatypes to columns in train_df\ntrain_df = train_df.astype(data_types_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all records of train_df in which interactions are questions\ntrain_df = train_df[train_df[target] != -1].reset_index(drop=True)\nprint(train_df)\n# Fill NAN places in \"prior_question_elapsed_time\" column with its mean value\nprior_question_elapsed_time_mean=train_df['prior_question_elapsed_time'].mean()\ntrain_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace=True)\nprint(train_df['prior_question_elapsed_time'])\n# Get maximum timestamp for each user\nmax_timestamp_u = train_df[['user_id','timestamp']].groupby(['user_id']).agg(['max']).reset_index()\nmax_timestamp_u.columns = ['user_id', 'max_time_stamp']\n# Get actualtimestamp of interaction by subtracting previous time from the current timestamp\ntrain_df['actualtimestamp'] = train_df.groupby('user_id')['timestamp'].shift()\ntrain_df['actualtimestamp']=train_df['timestamp']-train_df['actualtimestamp']\ntrain_df['actualtimestamp'].fillna(0, inplace=True)\ntrain_df.actualtimestamp=train_df.actualtimestamp.astype('int32')\nprint(train_df.actualtimestamp)\n# Get average of actualtimestamp for each user\nactualtimestamp_calc = train_df.groupby('user_id')['actualtimestamp'].agg(['mean'])\ntrain_df['actualtimestamp_avg'] = train_df['user_id'].map(actualtimestamp_calc['mean'])\ntrain_df.actualtimestamp_avg=train_df.actualtimestamp_avg.astype('int32')\nprint(train_df.actualtimestamp_avg)\n# Get \"actual_prior_question_elapsed_time\" by subtracting previous time from \"prior_question_elapsed_time\" of current interaction \nuser_prior_question_elapsed_time = train_df[['user_id','prior_question_elapsed_time']].groupby(['user_id']).tail(1)\nuser_prior_question_elapsed_time.columns = ['user_id', 'prior_question_elapsed_time']\ntrain_df['actual_prior_question_elapsed_time'] = train_df.groupby('user_id')['prior_question_elapsed_time'].shift()\ntrain_df['actual_prior_question_elapsed_time'] = train_df['prior_question_elapsed_time']-train_df['actual_prior_question_elapsed_time']\ntrain_df['actual_prior_question_elapsed_time'].fillna(0, inplace=True)\ntrain_df.actual_prior_question_elapsed_time=train_df.actual_prior_question_elapsed_time.astype('int32')\nprint(train_df.actual_prior_question_elapsed_time)\n# Convert miliseconds of timestamp into seconds\ntrain_df['timestamp']=train_df['timestamp']/(1000*3600)\ntrain_df.timestamp=train_df.timestamp.astype('int16')\nprint(train_df.timestamp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get correctness trend along the records of each user\ntrain_df['lag'] = train_df.groupby('user_id')[target].shift()\nmarks_trend = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\ntrain_df['user_correctness'] = marks_trend['cumsum'] / marks_trend['cumcount']\ntrain_df.user_correctness=train_df.user_correctness.astype('float16')\nprint(train_df.user_correctness)\n# Get comulative score of questions attempted along the records of each user\ntrain_df['user_cumulative_score'] = marks_trend['cumsum']\ntrain_df['user_cumulative_score'].fillna(0, inplace=True)\ntrain_df.user_cumulative_score=train_df.user_cumulative_score.astype('int16')\nprint(train_df.user_cumulative_score.head(n=50))\n# Get cumulative count of questions attempted along the records of each user\ntrain_df['user_cumulative_questions_count'] = marks_trend['cumcount']\ntrain_df.user_cumulative_questions_count=train_df.user_cumulative_questions_count.astype('int16')\nprint(train_df.user_cumulative_questions_count.head(n=50))\ntrain_df.drop(columns=['lag'], inplace=True)\n# Get average count of prior explored questions for each user_id\ntrain_df.prior_question_had_explanation=train_df.prior_question_had_explanation.astype('int8')\ntrain_df['lag'] = train_df.groupby('user_id')['prior_question_had_explanation'].shift()\nexploration_trend = train_df.groupby('user_id')['lag'].agg(['cumsum', 'cumcount'])\ntrain_df['avg_prior_exploration'] = exploration_trend['cumsum'] / exploration_trend['cumcount']\ntrain_df['avg_prior_exploration'].fillna(0, inplace=True)\ntrain_df.avg_prior_exploration=train_df.avg_prior_exploration.astype('float16')\nprint(train_df.avg_prior_exploration)\n# Get count of prior explored questions along the records of each user\ntrain_df['cumulative_exploration_count'] = exploration_trend['cumsum']\ntrain_df['cumulative_exploration_count'].fillna(0, inplace=True)\ntrain_df.cumulative_exploration_count=train_df.cumulative_exploration_count.astype('int16')\nprint(train_df.cumulative_exploration_count)\n\ntrain_df.drop(columns=['lag'], inplace=True)\ndel marks_trend\ndel exploration_trend\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get trend of attempts count for each interaction along the records of each user\ntrain_df[\"attempt_no\"] = 1\ntrain_df.attempt_no=train_df.attempt_no.astype('int8')\ntrain_df[\"attempt_no\"] = train_df[[\"user_id\",\"content_id\",'attempt_no']].groupby([\"user_id\",\"content_id\"])[\"attempt_no\"].cumsum()\nprint(train_df[\"attempt_no\"])\n# Get total attempts of each interaction for each user\ninteraction_total_attempts_calc=train_df.groupby([\"user_id\",\"content_id\"])[\"attempt_no\"].agg(['sum'])\ninteraction_total_attempts_calc=interaction_total_attempts_calc.astype('int8')\nprint(interaction_total_attempts_calc)\n# Get count of exploring answers (sum) for each user\nanswer_explored_calc = train_df.groupby('user_id')['prior_question_had_explanation'].agg(['sum', 'count'])\nanswer_explored_calc=answer_explored_calc.astype('int16')\nprint(answer_explored_calc)\n# Get total marks (sum) and total interactions (count) for each user\nuser_level_calc = train_df.groupby('user_id')[target].agg(['sum', 'count'])\nuser_level_calc=user_level_calc.astype('int16')\nprint(user_level_calc)\n# Get total marks (sum), total interactions (count) and marks variance (var) for each question_id\nquestion_level_calc = train_df.groupby('content_id')[target].agg(['sum', 'count','var'])\nquestion_level_calc = question_level_calc.astype('float32')\nprint(question_level_calc)\n# Get question level of easiness by dividing total score with total interactions\ntrain_df['question_interactions'] = train_df['content_id'].map(question_level_calc['count']).astype('int32')\ntrain_df['question_score'] = train_df['content_id'].map(question_level_calc['sum']).astype('int32')\ntrain_df['question_easiness'] = train_df['content_id'].map(question_level_calc['sum'] / question_level_calc['count'])\ntrain_df.question_easiness=train_df.question_easiness.astype('float16')\nprint(train_df.question_easiness)\n# Get total marks (sum), total interactions (count), and marks variance (var) for each batch of questions that consecutively attempt before exploring answers\nquestions_batch_calc = train_df.groupby('task_container_id')[target].agg(['sum', 'count','var'])\nquestions_batch_calc=questions_batch_calc.astype('float32')\nprint(questions_batch_calc)\n# Get easiness level of each batch of questions by dividing total score with total interactions\ntrain_df['questions_batch_score'] = train_df['task_container_id'].map(questions_batch_calc['sum']).astype('int32')\ntrain_df['questions_batch_score_variance'] = train_df['task_container_id'].map(questions_batch_calc['var']).astype('float16')\ntrain_df['questions_batch_easiness'] = train_df['task_container_id'].map(questions_batch_calc['sum'] / questions_batch_calc['count'])\ntrain_df.questions_batch_easiness=train_df.questions_batch_easiness.astype('float16')\nprint(train_df.questions_batch_easiness)\npd.set_option('display.max_columns', None)\ntrain_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get dataframe from questions.csv\nquestions_df = pd.read_csv(\n    '/kaggle/input/riiid-test-answer-prediction/questions.csv', \n    usecols=[0,1,3,4],\n    dtype={'question_id': 'int16','bundle_id': 'int16', 'part': 'int8','tags': 'str'})\n# Get questions belong to same part of test\nquestions_df['part_with_bundle_id']=questions_df['part']*100000+questions_df['bundle_id']\nquestions_df.part_with_bundle_id=questions_df.part_with_bundle_id.astype('int32')\n# Split each tag of question into sub-tags\ntag = questions_df[\"tags\"].str.split(\" \", n = 10, expand = True)\ntag.columns = ['tags1','tags2','tags3','tags4','tags5','tags6']\ntag.fillna(0, inplace=True)\ntag = tag.astype('int16')\n# Remove \"tags\" column from questions_df\nquestions_df =  pd.concat([questions_df,tag],axis=1).drop(['tags'],axis=1)\n# Rename question_id with content_id\nquestions_df.rename(columns={'question_id':'content_id'}, inplace=True)\n# Get average score of user for each question\nquestions_df['question_easiness'] = questions_df['content_id'].map(question_level_calc['sum'] / question_level_calc['count'])\nquestions_df.question_easiness=questions_df.question_easiness.astype('float16')\nprint(questions_df.question_easiness)\n# Get score variance of user for each question\nquestions_df['question_easiness_variance'] = questions_df['content_id'].map(question_level_calc['var'])\nquestions_df.question_easiness_variance=questions_df.question_easiness_variance.astype('float16')\nprint(questions_df.question_easiness_variance)\n# Get average score of user for questions of each test part\npart_easiness_calc = questions_df.groupby('part')['question_easiness'].agg(['mean', 'var'])\nquestions_df['avg_part_easiness'] = questions_df['part'].map(part_easiness_calc['mean'])\nquestions_df.avg_part_easiness=questions_df.avg_part_easiness.astype('float16')\nprint(questions_df.avg_part_easiness)\n# Get score variance of user for questions of each test part\nquestions_df['part_easiness_variance'] = questions_df['part'].map(part_easiness_calc['var'])\nquestions_df.part_easiness_variance=questions_df.part_easiness_variance.astype('float16')\nprint(questions_df.part_easiness_variance)\n# Get average score of user for questions of each bundle_id\nbundle_easiness_calc = questions_df.groupby('bundle_id')['question_easiness'].agg(['mean'])\nquestions_df['bundle_correctness'] = questions_df['bundle_id'].map(bundle_easiness_calc['mean'])\nquestions_df.bundle_correctness=questions_df.bundle_correctness.astype('float16')\nprint(questions_df.bundle_correctness)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get average score of user for questions of subtag column \"tags1\"\ntags1_easiness_calc = questions_df.groupby('tags1')['question_easiness'].agg(['mean', 'var'])\nquestions_df['avg_tags1_easiness'] = questions_df['tags1'].map(tags1_easiness_calc['mean'])\nquestions_df.avg_tags1_easiness=questions_df.avg_tags1_easiness.astype('float16')\nprint(questions_df.avg_tags1_easiness)\n# Get score variance of user for questions of subtag column \"tags1\"\nquestions_df['tags1_easiness_variance'] = questions_df['tags1'].map(tags1_easiness_calc['var'])\nquestions_df.tags1_easiness_variance=questions_df.tags1_easiness_variance.astype('float16')\nprint(questions_df.tags1_easiness_variance)\n# Drop questions_df column \"question_easiness\" and delete \"bundle_easiness_calc\", \"part_easiness_calc\", and \"tags1_easiness_calc\"\nquestions_df.drop(columns=['question_easiness'], inplace=True)\ndel bundle_easiness_calc\ndel part_easiness_calc\ndel tags1_easiness_calc\ngc.collect()\n# Fill NAN places of train_df columns \"user_correctness\" and \"attempt_no\" with 1 and remaining with 0 \ntrain_df['user_correctness'].fillna( 1, inplace=True)\ntrain_df['attempt_no'].fillna(1, inplace=True)\ntrain_df.fillna(0, inplace=True)\n# Pring datatypes of all columns in questions_df\nprint(questions_df.dtypes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Self Attention Knowledge Tracing Model (Part 1)**","metadata":{}},{"cell_type":"code","source":"# Parameter specifications for self-attention knowledge tracing model\n\nMAX_SEQ = 160 # maximum number of questions to be encountered for a user\nquests = train_df[\"content_id\"].unique() # List of unique question_ids in train_df\nn_quests = len(quests) # number of unique questions\nprint(n_quests)\n# Get question_ids and respective scores as two consecutive lists for each user\ngroup = train_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n            r['content_id'].values,\n            r['answered_correctly'].values))\nprint(group)\n# Check if number of questions is less than or equal to 160\nfor user_id in group.index:\n    q, qa = group[user_id]\n    if len(q)>MAX_SEQ:\n        group[user_id] = (q[-MAX_SEQ:],qa[-MAX_SEQ:])\n        \n# Save user scores and attempted questions information into a file \"group.pkl\"       \n#pickle.dump(group, open(\"group.pkl\", \"wb\"))\n\ndel group\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"code","source":"features = [\n    'actualtimestamp',\n    'actualtimestamp_avg',\n    'cumulative_user_lectures', \n    'user_lecture_interaction_pattern',\n    'prior_question_elapsed_time',\n    'actual_prior_question_elapsed_time',\n    'user_correctness',\n    'user_cumulative_questions_count', \n    'user_cumulative_score', \n    'question_easiness',\n    'question_interactions',\n    'question_score', \n    'questions_batch_easiness',\n    'bundle_correctness',\n    'attempt_no',\n    'part',\n    'avg_part_easiness',\n    'tags1',\n    'avg_tags1_easiness',\n    'bundle_id',\n    'avg_prior_exploration', \n    'cumulative_exploration_count',\n    'prior_question_had_explanation',\n]\ncategorical_columns= [\n    'part',        \n    'tags1',\n    'bundle_id',\n    'prior_question_had_explanation',\n]\nflag_lightgbm=True\nclassifiers = list()\n# A dictionary of specified parameters\nparameters = {\n'num_leaves': 350,\n'max_bin':700,\n'min_child_weight': 0.03454472573214212,\n'feature_fraction': 0.58,\n'bagging_fraction': 0.58,\n'objective': 'binary',\n'max_depth': -1,\n'learning_rate': 0.05,\n\"boosting_type\": \"gbdt\",\n\"bagging_seed\": 11,\n\"metric\": 'auc',\n\"verbosity\": -1,\n'reg_alpha': 0.3899927210061127,\n'reg_lambda': 0.6485237330340494,\n'random_state': 47\n}\ntrains=list()\nvalids=list()\nnum=1\nfor i in range(0,num):\n    train_df_clf=train_df.sample(n=20000*1000) # Get sample from train_df\n    del train_df\n    users=train_df_clf['user_id'].drop_duplicates() # Get unique user_ids \n    users=users.sample(frac=0.025) # Get a fraction of user from sample users\n    users_df=pd.DataFrame() # Create a dataframe for fractional set of users\n    users_df['user_id']=users.values\n    # Merge sample dataframe (train_df_clf) with fractional users dataframe (users_df) based on user_id\n    valid_df_newuser = pd.merge(train_df_clf, users_df, on=['user_id'], how='inner',right_index=True)\n    del users_df\n    del users\n    gc.collect()\n    train_df_clf.drop(valid_df_newuser.index, inplace=True)\n    # Merge sample dataframe (train_df_clf) with questions_df based on question_id\n    train_df_clf = pd.merge(train_df_clf, questions_df, on='content_id', how='left',right_index=True)\n    # Merge sample dataframe (valid_df_newuser) wit hquestions_df based on question_id\n    valid_df_newuser = pd.merge(valid_df_newuser, questions_df, on='content_id', how='left',right_index=True)\n    valid_df=train_df_clf.sample(frac=0.09)\n    train_df_clf.drop(valid_df.index, inplace=True)\n    valid_df = valid_df.append(valid_df_newuser)\n    del valid_df_newuser\n    gc.collect()\n    trains.append(train_df_clf) # Training data\n    valids.append(valid_df) # Validation data\ndel train_df_clf\ndel valid_df\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,num):\n    # Create dataset object of lightgbm containing features and labels of train data instances\n    train_data = lgb.Dataset(trains[i][features], label=trains[i][target])  \n    # Create dataset object of lightgbm containing features and labels of validation data instances\n    validation_data = lgb.Dataset(valids[i][features], label=valids[i][target]) \n    del trains\n    del valids\n    gc.collect()\n    \n# Build model and save it\n#   model = lgb.train(\n#         parameters, \n#         train_data,\n#         num_boost_round=4000,\n#         valid_sets=[train_data, validation_data],\n#         early_stopping_rounds=40,\n#         feature_name=features,\n#         categorical_feature=categorical_columns,\n#         verbose_eval=40\n#   )\n#   classifiers.append(model)\n#pickle.dump(classifiers, open(\"classifiers.pkl\", \"wb\"))\n\n    # Load trained lightGBM model \n    classifiers_file = \"../input/lightgbm-modeling/classifiers.pkl\"\n    classifiers = pickle.load(open(classifiers_file, \"rb\"))\n    # Plot level of importance for different features extracted while pre-processing\n    lgb.plot_importance(classifiers[0], importance_type='gain')\n    plt.show()\n    del train_data\n    del validation_data\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Inferences**","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n# Get dictionaries of all features extracted from dataset\nuser_correctness_dict = user_level_calc['sum'].astype('int16').to_dict(defaultdict(int))\nuser_activeness_dict = user_level_calc['count'].astype('int16').to_dict(defaultdict(int))\nquestion_score_dict = question_level_calc['sum'].astype('int32').to_dict(defaultdict(int))\nquestion_interactions_dict = question_level_calc['count'].astype('int32').to_dict(defaultdict(int))\ndel user_level_calc\ndel question_level_calc\ngc.collect()\nquestions_batch_score_dict = questions_batch_calc['sum'].astype('int32').to_dict(defaultdict(int))\nquestions_batch_count_dict = questions_batch_calc['count'].astype('int32').to_dict(defaultdict(int))\nquestions_batch_score_variance_dict = questions_batch_calc['var'].astype('float16').to_dict(defaultdict(int))\nexplanation_sum_dict = answer_explored_calc['sum'].astype('int16').to_dict(defaultdict(int))\nexplanation_count_dict = answer_explored_calc['count'].astype('int16').to_dict(defaultdict(int))\ndel questions_batch_calc\ndel answer_explored_calc\ngc.collect()\nuser_lecture_sum_dict = user_lecture_aggregate['sum'].astype('int16').to_dict(defaultdict(int))\nuser_lecture_count_dict = user_lecture_aggregate['count'].astype('int16').to_dict(defaultdict(int))\nactualtimestamp_avg_dict = actualtimestamp_calc['mean'].astype('int32').to_dict(defaultdict(int))\ndel user_lecture_aggregate\ndel actualtimestamp_calc\ngc.collect()\ninteraction_total_attempts_calc=interaction_total_attempts_calc[interaction_total_attempts_calc['sum'] >1]\nattempt_no_sum_dict = interaction_total_attempts_calc['sum'].to_dict(defaultdict(int))\ndel interaction_total_attempts_calc\ngc.collect()\nmax_timestamp_u_dict=max_timestamp_u.set_index('user_id').to_dict()\nuser_prior_question_elapsed_time_dict=user_prior_question_elapsed_time.set_index('user_id').to_dict()\ndel max_timestamp_u\ndel user_prior_question_elapsed_time\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get maximum count of attempts for each question of user\ndef get_max_attempt(user_id,content_id): \n    k = (user_id,content_id)\n    if k in attempt_no_sum_dict.keys():\n        attempt_no_sum_dict[k]+=1\n        return attempt_no_sum_dict[k]\n    attempt_no_sum_dict[k] = 1\n    return attempt_no_sum_dict[k]\nprint(psutil.virtual_memory().percent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Self Attention Knowledge Tracing Model (Part 2)**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Create class for feed-forward neural network\nclass FFN(nn.Module):\n    \n    # init() function defines architectural specifications of feed-forward neural network\n    def __init__(self, state_size=200):\n        super(FFN, self).__init__()\n        self.state_size = state_size # Specifies number of features\n        self.lr1 = nn.Linear(state_size, state_size) # First linear layer with 200 features in and 200 features out\n        self.relu = nn.ReLU() # Rectified linear activation function\n        self.lr2 = nn.Linear(state_size, state_size) # Second linear layer with 200 features in and 200 features out\n        self.dropout = nn.Dropout(0.2) # Last dropout layer that drops number of nodes in a layer\n    \n    # forward() function defines functionality of feed-forward neural network\n    def forward(self, x):\n        x = self.lr1(x) \n        x = self.relu(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n    \ndef future_mask(seq_length):\n    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n    return torch.from_numpy(future_mask)\n\n# Create class for self-attention knowledge tracing model\nclass SAKT_Model(nn.Module):\n    \n    # init() fuction defines architectural specifications of self-attention knowledge tracing model\n    def __init__(self, n_quests, max_seq=MAX_SEQ, embed_dim=128): \n        super(SAKT_Model, self).__init__()\n        self.n_quests = n_quests # Specifying number of vectors\n        self.embed_dim = embed_dim # Specifying length of each vector\n        self.embedding = nn.Embedding(2*n_quests+1, embed_dim) # Embeddings of vectors that are processed\n        self.pos_embedding = nn.Embedding(max_seq-1, embed_dim) # Positional embeddings of vectors that track vectors ordering \n        self.e_embedding = nn.Embedding(n_quests+1, embed_dim) # Embeddings of vectors that pass as residual\n       \n        # Multiple heads indicate multiple vectors are processed simultaneously by self-attention mechanism\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        # In self-attention block, self-attention layer is followed by normalization layer\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        # Normalized output is fed to feed-forward neural network\n        self.ffn = FFN(embed_dim)\n        # Linear layer concatenates output of all heads to give a single output\n        self.pred = nn.Linear(embed_dim, 1)\n            \n    # forward() function defines functionality of self-attention knowledge tracing model\n    def forward(self, x, question_ids):\n        device = x.device     \n        # Get embeddings of x vector\n        x = self.embedding(x)       \n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        # Get positional embeddings of x vector\n        pos_x = self.pos_embedding(pos_id)\n        # Join both vector and positional embeddings\n        x = x + pos_x\n        # Get residual of vector embeddings\n        e = self.e_embedding(question_ids)\n        # Swap first two dimensions of tensors x and e\n        x = x.permute(1, 0, 2) \n        e = e.permute(1, 0, 2)\n        \n        att_mask = future_mask(x.size(0)).to(device)\n        # Self-attention layer is applied that outputs contextualized embeddings of vectors and their weight matrices\n        att_output, att_weight = self.multi_att(e, x, x, attn_mask=att_mask)\n        # Normalize the contextualized vector and residual vector\n        att_output = self.layer_normal(att_output + e)\n        # Swap first two dimensions of normalized output \n        att_output = att_output.permute(1, 0, 2) \n        # Input swapped-output to feed-forward neural network\n        x = self.ffn(att_output)\n        # Normalize the output of feed-forward neural network and residual of contexualized vector\n        x = self.layer_normal(x + att_output)\n        # Concatenate all contextualized vectors to a single vector\n        x = self.pred(x)\n        # Returns concatenated contextualized vector and weight matrix\n        return x.squeeze(-1), att_weight\n\n# Create class for test dataset\nclass TestDataset(Dataset):\n    \n    # init() function defines test data specifications\n    def __init__(self, samples, test_df, quests, max_seq=MAX_SEQ): \n        super(TestDataset, self).__init__()\n        self.samples = samples # Subset of test data\n        self.user_ids = [x for x in test_df[\"user_id\"].unique()] # List of user IDs in test data\n        self.test_df = test_df # Entire test data\n        self.quests = quests # List of user interactions\n        self.n_quests = len(quests) # Count of user interactions\n        self.max_seq = max_seq # Maximum count of user interactions\n\n    # len() function provides rows count in test data\n    def __len__(self):\n        return self.test_df.shape[0]\n\n    # getitem() function provides information about each item of test data \n    def __getitem__(self, index):\n        # Get user_id and content_id of each test instance\n        test_info = self.test_df.iloc[index]\n        user_id = test_info[\"user_id\"]\n        target_id = test_info[\"content_id\"]\n        # If instance/user is found in samples then get his all interactions (q_) and score (qa_)\n        q = np.zeros(self.max_seq, dtype=int)\n        qa = np.zeros(self.max_seq, dtype=int)\n        if user_id in self.samples.index:\n            q_, qa_ = self.samples[user_id]\n            # Check if interactions count (q_) is less than or equal to maximum interaction count (max_seq)        \n            seq_len = len(q_)\n            if seq_len >= self.max_seq:\n                q = q_[-self.max_seq:]\n                qa = qa_[-self.max_seq:]\n            else:\n                q[-seq_len:] = q_\n                qa[-seq_len:] = qa_          \n        # Make a copy of user interactions \n        x = np.zeros(self.max_seq-1, dtype=int)\n        x = q[1:].copy()\n        x += (qa[1:] == 1) * self.n_quests\n        \n        questions = np.append(q[2:], [target_id])\n        \n        return x, questions\n# Device specification to run torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Build self-attention knowledge tracing model \nself_attention_knowledge_tracing_model = SAKT_Model(n_quests, embed_dim=128)\n# Load tensor parameters for modeling of self-attention knowledge tracing\ntry:\n    self_attention_knowledge_tracing_model.load_state_dict(torch.load(\"../input/sakt-modeling/SAKT-HDKIM.pt\"))\nexcept:\n    self_attention_knowledge_tracing_model.load_state_dict(torch.load(\"../input/sakt-modeling/SAKT-HDKIM.pt\", map_location='cpu'))\n    \n# Run model on device\nself_attention_knowledge_tracing_model.to(device)\n# Evaluate the model\nself_attention_knowledge_tracing_model.eval()\n# Load file \"group_pkl\"\ngroup_file = \"../input/correctness-trend/group.pkl\"\ngroup = pickle.load(open(group_file, \"rb\"))\nprint(psutil.virtual_memory().percent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing**","metadata":{}},{"cell_type":"code","source":"%%time\nenv = riiideducation.make_env()\niter_test = env.iter_test()\nprior_test_df = None\n\nfor (test_df, sample_prediction_df) in iter_test:  \n    if (prior_test_df is not None) & (psutil.virtual_memory().percent<90):\n        # Evaluate label of test_df first instance\n        prior_test_df[target] = eval(test_df['prior_group_answers_correct'].iloc[0])\n        # Get prior_test_df containing data of question interactions only\n        prior_test_df = prior_test_df[prior_test_df[target] != -1].reset_index(drop=True) \n        # Fill in NAN places of prior_test_df column \"prior_question_had_explanation\" with False\n        prior_test_df['prior_question_had_explanation'].fillna(False, inplace=True)       \n        prior_test_df.prior_question_had_explanation=prior_test_df.prior_question_had_explanation.astype('int8')\n        # Get two consecutive lists of question ids and their respective scores for each user_id in prior_test_df\n        prev_group = prior_test_df[['user_id', 'content_id', 'answered_correctly']].groupby('user_id').apply(lambda r: (\n            r['content_id'].values,\n            r['answered_correctly'].values))\n        for prev_user_id in prev_group.index:\n            prev_group_content = prev_group[prev_user_id][0] # Question id\n            prev_group_ac = prev_group[prev_user_id][1] # Score\n            # Get tuples of question_id and scores for each user_id of prev_group that is found in group \n            if prev_user_id in group.index:\n                group[prev_user_id] = (np.append(group[prev_user_id][0],prev_group_content), \n                                       np.append(group[prev_user_id][1],prev_group_ac))\n            else:\n                group[prev_user_id] = (prev_group_content,prev_group_ac)\n            # Check if questions count is less than or equal to MAX_SEQ=160\n            if len(group[prev_user_id][0])>MAX_SEQ:\n                new_group_content = group[prev_user_id][0][-MAX_SEQ:]\n                new_group_ac = group[prev_user_id][1][-MAX_SEQ:]\n                group[prev_user_id] = (new_group_content,new_group_ac)\n        # Get values of some columns from \"prior_test_df\" to create their dictionaries  \n        user_ids = prior_test_df['user_id'].values\n        content_ids = prior_test_df['content_id'].values\n        task_container_ids = prior_test_df['task_container_id'].values\n        prior_question_had_explanations = prior_test_df['prior_question_had_explanation'].values\n        targets = prior_test_df[target].values    \n        for user_id, content_id,prior_question_had_explanation,task_container_id,answered_correctly in zip(user_ids, content_ids, prior_question_had_explanations,task_container_ids,targets):\n            user_correctness_dict[user_id] += answered_correctly\n            user_activeness_dict[user_id] += 1         \n            explanation_sum_dict[user_id] += prior_question_had_explanation\n            explanation_count_dict[user_id] += 1\n            \n    # Create a copy of test_df\n    prior_test_df = test_df.copy()\n    # Get test_df where interactions are only lectures\n    lecture_test_df = test_df[test_df['content_type_id'] == 1].reset_index(drop=True)\n    # Get total count of lectures interactions by each user_id\n    for i, (user_id,content_type_id, content_id) in enumerate(zip(lecture_test_df['user_id'].values,lecture_test_df['content_type_id'].values,lecture_test_df['content_id'].values)):\n        user_lecture_sum_dict[user_id] += content_type_id\n        user_lecture_count_dict[user_id] += 1\n        if(len(user_lecture_interactions[user_lecture_interactions.user_id==user_id])==0):\n            user_lecture_interactions = user_lecture_interactions.append([{'user_id':user_id}], ignore_index=True)\n            user_lecture_interactions.fillna(0, inplace=True)\n            user_lecture_interactions.loc[user_lecture_interactions.user_id==user_id,lectures_top_categories + lectures_basic_purposes]+=lectures_df[lectures_df.lecture_id==content_id][lectures_top_categories + lectures_basic_purposes].values\n        else:\n            user_lecture_interactions.loc[user_lecture_interactions.user_id==user_id,lectures_top_categories + lectures_basic_purposes]+=lectures_df[lectures_df.lecture_id==content_id][lectures_top_categories + lectures_basic_purposes].values   \n    # Get test_df where interactions are only questions\n    test_df = test_df[test_df['content_type_id'] == 0].reset_index(drop=True)\n    # Get test data and load its batch wise\n    test_dataset = TestDataset(group, test_df, quests)\n    test_dataloader = DataLoader(test_dataset, batch_size=51200, shuffle=False)\n    \n    self_attention_knowledge_tracing_outs = []\n    # Get an instance from batch of test data\n    for item in test_dataloader:\n        x = item[0].to(device).long() # Features of test instance\n        target_id = item[1].to(device).long() # Lable of test instance\n        # Run self_attention_knowledge_tracing model to get prediction for given instance\n        with torch.no_grad():\n            output, att_weight = self_attention_knowledge_tracing_model(x, target_id)\n        output = torch.sigmoid(output)\n        output = output[:, -1]\n        self_attention_knowledge_tracing_outs.extend(output.view(-1).data.cpu().numpy())\n    # Fill NAN places in test_df column \"prior_question_had_explanation\" with False and \"prior_question_elapsed_time\" with its mean value\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df.prior_question_had_explanation=test_df.prior_question_had_explanation.astype('int8')\n    test_df['prior_question_elapsed_time'].fillna(prior_question_elapsed_time_mean, inplace=True)\n    # Initialization of variable arrays to get features\n    user_lecture_sum = np.zeros(len(test_df), dtype=np.int16)\n    user_lecture_count = np.zeros(len(test_df), dtype=np.int16)    \n    user_sum = np.zeros(len(test_df), dtype=np.int16)\n    user_count = np.zeros(len(test_df), dtype=np.int16)\n    question_score = np.zeros(len(test_df), dtype=np.int32)\n    question_interactions = np.zeros(len(test_df), dtype=np.int32)\n    questions_batch_score = np.zeros(len(test_df), dtype=np.int32)\n    questions_batch_count = np.zeros(len(test_df), dtype=np.int32)\n    questions_batch_score_variance = np.zeros(len(test_df), dtype=np.float16)\n    content_task_mean = np.zeros(len(test_df), dtype=np.float16)\n    explanation_sum = np.zeros(len(test_df), dtype=np.int32)\n    explanation_count = np.zeros(len(test_df), dtype=np.int32)\n    actual_prior_question_elapsed_time = np.zeros(len(test_df), dtype=np.int32)\n    attempt_no_count = np.zeros(len(test_df), dtype=np.int16)\n    actualtimestamp = np.zeros(len(test_df), dtype=np.int32)\n    actualtimestamp_avg = np.zeros(len(test_df), dtype=np.int32)  \n    # Get features of each user from predefined dictionaries\n    for i, (user_id,prior_question_had_explanation,content_type_id,prior_question_elapsed_time,timestamp, \n            content_id,task_container_id) in enumerate(zip(test_df['user_id'].values,\n            test_df['prior_question_had_explanation'].values,test_df['content_type_id'].values,\n            test_df['prior_question_elapsed_time'].values,test_df['timestamp'].values, test_df['content_id'].values,\n            test_df['task_container_id'].values)):         \n        user_lecture_sum_dict[user_id] += content_type_id\n        user_lecture_count_dict[user_id] += 1        \n        user_lecture_sum[i] = user_lecture_sum_dict[user_id]\n        user_lecture_count[i] = user_lecture_count_dict[user_id]        \n        user_sum[i] = user_correctness_dict[user_id]\n        user_count[i] = user_activeness_dict[user_id]\n        question_score[i] = question_score_dict[content_id]\n        question_interactions[i] = question_interactions_dict[content_id]\n        questions_batch_score[i] = questions_batch_score_dict[task_container_id]\n        questions_batch_count[i] = questions_batch_count_dict[task_container_id]\n        questions_batch_score_variance[i]=questions_batch_score_variance_dict[task_container_id]  \n        explanation_sum[i] = explanation_sum_dict[user_id]\n        explanation_count[i] = explanation_count_dict[user_id]\n        # Get actualtimestamp by subtracting cumulative previous time from timestamp of an interaction\n        if user_id in max_timestamp_u_dict['max_time_stamp'].keys():\n            actualtimestamp[i]=timestamp-max_timestamp_u_dict['max_time_stamp'][user_id]\n            max_timestamp_u_dict['max_time_stamp'][user_id]=timestamp\n            actualtimestamp_avg[i]=(actualtimestamp_avg_dict[user_id]+actualtimestamp[i])/2           \n        else:\n            actualtimestamp[i]=0\n            max_timestamp_u_dict['max_time_stamp'].update({user_id:timestamp})\n            actualtimestamp_avg_dict.update({user_id:timestamp})\n            actualtimestamp_avg[i]=(actualtimestamp_avg_dict[user_id]+actualtimestamp[i])/2\n        # Get actual_prior_question_elapsed_time by subtracting cumulative previous time from prior_question_elapsed_time of an interaction\n        if user_id in user_prior_question_elapsed_time_dict['prior_question_elapsed_time'].keys():            \n            actual_prior_question_elapsed_time[i]=prior_question_elapsed_time-user_prior_question_elapsed_time_dict['prior_question_elapsed_time'][user_id]\n            user_prior_question_elapsed_time_dict['prior_question_elapsed_time'][user_id]=prior_question_elapsed_time\n        else:           \n            actual_prior_question_elapsed_time[i]=0    \n            user_prior_question_elapsed_time_dict['prior_question_elapsed_time'].update({user_id:prior_question_elapsed_time})\n    # Merge test_df with questions_df based on \"content_id\" between two dataframes\n    test_df=test_df.merge(questions_df.loc[questions_df.index.isin(test_df['content_id'])], how='left', on='content_id', right_index=True)\n    # Define features added to test_df\n    test_df['user_lecture_interaction_pattern'] = user_lecture_sum / user_lecture_count\n    test_df['cumulative_user_lectures'] = user_lecture_sum\n    test_df['user_correctness'] = user_sum / user_count\n    test_df['user_cumulative_questions_count'] =user_count\n    test_df['user_cumulative_score'] =user_sum\n    test_df['question_easiness'] = question_score / question_interactions\n    test_df['question_interactions'] = question_interactions\n    test_df['question_score'] = question_score\n    test_df['questions_batch_easiness'] = questions_batch_score / questions_batch_count\n    test_df['questions_batch_score'] = questions_batch_score \n    test_df['questions_batch_score_variance'] = questions_batch_score_variance \n    test_df['avg_prior_exploration'] = explanation_sum / explanation_count\n    test_df['cumulative_exploration_count'] = explanation_sum  \n    test_df['actual_prior_question_elapsed_time'] = actual_prior_question_elapsed_time \n    test_df[\"attempt_no\"] = test_df[[\"user_id\", \"content_id\"]].apply(lambda row: get_max_attempt(row[\"user_id\"], row[\"content_id\"]), axis=1)\n    test_df[\"actualtimestamp\"]=actualtimestamp\n    test_df[\"actualtimestamp_avg\"]=actualtimestamp_avg\n    # Fill NAN places of test_df columns \"user_correctness\" and \"attempt_no\" with 1 and in remaining columns with 0\n    test_df['user_correctness'].fillna( 1, inplace=True)\n    test_df['attempt_no'].fillna(1, inplace=True)\n    test_df.fillna(0, inplace=True)\n    # Convert values of timestamp from miliseconds to seconds \n    test_df['timestamp']=test_df['timestamp']/(1000*3600)\n    test_df.timestamp=test_df.timestamp.astype('int16')\n    # Get prediction score of each test_df instance (test_preds) that add up to get prediction score (sub_preds) for each classifier\n    sub_preds = np.zeros(test_df.shape[0])\n    for i, model in enumerate(classifiers, 1):\n        test_preds  = model.predict(test_df[features])\n        sub_preds += test_preds\n    # Get average prediction for given \"row_id\"\n    lightgbm_final = sub_preds / len(classifiers) \n    test_df[target] = np.array(self_attention_knowledge_tracing_outs) * 0.5 + lightgbm_final * 0.5  \n    rows = test_df[target].to_csv()\n    filename = \"submission.csv\"\n    with open(filename, 'w') as csvfile: \n        csvwriter = csv.writer(csvfile)\n        csvwriter.writerows(rows)        \n    env.predict(test_df[['row_id', target]])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}